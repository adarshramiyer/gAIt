{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explore Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counting summary aspects of data\n",
    "\n",
    "video_data = os.listdir('..\\\\data_scraping\\\\Web_Scrape_Labeled_2\\\\')\n",
    "video_data = video_data + os.listdir('..\\\\data_supplement\\\\Self_Record_Labeled\\\\')\n",
    "# print(video_data)\n",
    "\n",
    "o_counter = 0\n",
    "f_counter = 0\n",
    "b_counter = 0\n",
    "s_counter = 0\n",
    "l_counter = 0\n",
    "\n",
    "left_counter = 0\n",
    "NUM_FILES = 76\n",
    "\n",
    "for filename in video_data:\n",
    "    o_counter += int(filename[4])\n",
    "    f_counter += int(filename[7])\n",
    "    b_counter += int(filename[10])\n",
    "    s_counter += int(filename[13])\n",
    "    l_counter += int(filename[16])\n",
    "    if (filename[18] == 'L'): \n",
    "        left_counter += 1\n",
    "    \n",
    "    # making sure labels are OK after averages\n",
    "    if (int(filename[7]) > 0 and int(filename[10]) > 0):\n",
    "        print('Review ' + str(filename))\n",
    "\n",
    "left_counter = float(left_counter) / (float(NUM_FILES))\n",
    "left_counter *= 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying summative data information\n",
    "\n",
    "print('L entrances: ' + str(round(left_counter, 3)) + '%')\n",
    "print('R entrances: ' + str(round(100 - left_counter, 3)) + '%\\n')\n",
    "\n",
    "print('Sums of all ratings: ')\n",
    "print('overstride: ' + str(o_counter))\n",
    "print('forward lean: ' + str(f_counter))\n",
    "print('backward lean: ' + str(b_counter))\n",
    "print('sweeping: ' + str(s_counter))\n",
    "print('low arms: ' + str(l_counter))\n",
    "\n",
    "errors = ['overstride', 'forward lean', 'backward lean', 'sweeping', 'low arms']\n",
    "rating_sums = [o_counter, f_counter, b_counter, s_counter, l_counter]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(errors, rating_sums, width = 0.5)\n",
    "plt.xlabel('Error Type')\n",
    "plt.ylabel('Sum of Error Ratings')\n",
    "plt.title('Sums of Error Ratings across 207 Web-Scraped Training Examples')    \n",
    "\n",
    "for index, value in enumerate(rating_sums):\n",
    "    plt.text(index - 0.08, value -50, str(value), color='white') # add annotations\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "type_count = np.zeros((5, 10), dtype=int)\n",
    "\n",
    "for filename in video_data:\n",
    "    type_count[0][int(filename[4])] += 1\n",
    "    type_count[1][int(filename[7])] += 1\n",
    "    type_count[2][int(filename[10])] += 1\n",
    "    type_count[3][int(filename[13])] += 1\n",
    "    type_count[4][int(filename[16])] += 1\n",
    "\n",
    "# print(type_count)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(type_count, cmap='plasma')\n",
    "\n",
    "ax.set_xticks(np.arange(len(ratings)))\n",
    "ax.set_yticks(np.arange(len(errors)))\n",
    "ax.set_xticklabels(ratings)\n",
    "ax.set_yticklabels(errors)\n",
    "\n",
    "for i in range(len(errors)):\n",
    "    for j in range(len(ratings)):\n",
    "        text = ax.text(j, i, type_count[i, j], ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use MoveNet to Save Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing keypoints\n",
    "def draw_keypoints(frame, keypoints, min_confidence):\n",
    "    y, x, c = frame.shape\n",
    "    print ('height: ' + str(y) + ' width: ' + str(x))\n",
    "    print('KEYPOINTS:')\n",
    "    print(keypoints)\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [1, 1, 1]))\n",
    "\n",
    "    print(shaped[0])\n",
    "\n",
    "    for kp in shaped:\n",
    "        ky, kx, conf = kp\n",
    "        print('KY, KX, CONF: ' + str(int(ky * y)) + ' ' +  str(int(kx * x)) + ' ' + str(conf))\n",
    "        if conf > min_confidence:\n",
    "            cv2.circle(frame, (int(kx * x), int(ky * y)), 5, (255, 0, 0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose estimation\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_thunder_tflite_float16_4.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "filename = '..\\\\data_scraping\\\\Web_Scrape_Labeled_2\\\\WS-O3-F1-B0-S4-L1-L-0166.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    red = frame[:,:,2].copy()\n",
    "    blue = frame[:,:,0].copy()\n",
    "    frame[:,:,2] = blue\n",
    "    frame[:,:,0] = red\n",
    "\n",
    "    if ret:\n",
    "    \n",
    "        # reshape image\n",
    "        img = frame.copy()\n",
    "        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 256,256)\n",
    "        input_image = tf.cast(img, dtype=tf.uint8) # change base on lightning/thunder float32/uint8\n",
    "        \n",
    "        # set up input / output\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        # make prediction\n",
    "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "        interpreter.invoke()\n",
    "        points_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        print(points_with_scores)\n",
    "\n",
    "        # display prediction\n",
    "        square_img = np.array(tf.cast(np.squeeze(img), dtype=tf.uint8))\n",
    "\n",
    "        square_frame = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 1280,1280)\n",
    "        square_frame = np.array(tf.cast(np.squeeze(square_frame), dtype=tf.uint8))\n",
    "\n",
    "\n",
    "        draw_keypoints(square_frame, points_with_scores, 0.1)  \n",
    "        \n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(square_frame) # display only\n",
    "\n",
    "        cap.release()     \n",
    "    \n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59acfcc823b36a638b37cb1c7843aa46684cb4b3e7f7aef341e5384d13f48e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
