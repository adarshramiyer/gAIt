{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explore Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "import mediapipe as mp\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counting summary aspects of data\n",
    "\n",
    "video_data = os.listdir('..\\\\data_scraping\\\\Web_Scrape_Labeled_2\\\\')\n",
    "video_data = video_data + os.listdir('..\\\\data_supplement\\\\Self_Record_Labeled\\\\')\n",
    "# print(video_data)\n",
    "\n",
    "o_counter = 0\n",
    "f_counter = 0\n",
    "b_counter = 0\n",
    "s_counter = 0\n",
    "l_counter = 0\n",
    "\n",
    "left_counter = 0\n",
    "NUM_FILES = 76\n",
    "\n",
    "for filename in video_data:\n",
    "    o_counter += int(filename[4])\n",
    "    f_counter += int(filename[7])\n",
    "    b_counter += int(filename[10])\n",
    "    s_counter += int(filename[13])\n",
    "    l_counter += int(filename[16])\n",
    "    if (filename[18] == 'L'): \n",
    "        left_counter += 1\n",
    "    \n",
    "    # making sure labels are OK after averages\n",
    "    if (int(filename[7]) > 0 and int(filename[10]) > 0):\n",
    "        print('Review ' + str(filename))\n",
    "\n",
    "left_counter = float(left_counter) / (float(NUM_FILES))\n",
    "left_counter *= 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying summative data information\n",
    "\n",
    "print('L entrances: ' + str(round(left_counter, 3)) + '%')\n",
    "print('R entrances: ' + str(round(100 - left_counter, 3)) + '%\\n')\n",
    "\n",
    "print('Sums of all ratings: ')\n",
    "print('overstride: ' + str(o_counter))\n",
    "print('forward lean: ' + str(f_counter))\n",
    "print('backward lean: ' + str(b_counter))\n",
    "print('sweeping: ' + str(s_counter))\n",
    "print('low arms: ' + str(l_counter))\n",
    "\n",
    "errors = ['overstride', 'forward lean', 'backward lean', 'sweeping', 'low arms']\n",
    "rating_sums = [o_counter, f_counter, b_counter, s_counter, l_counter]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.bar(errors, rating_sums, width = 0.5)\n",
    "plt.xlabel('Error Type')\n",
    "plt.ylabel('Sum of Error Ratings')\n",
    "plt.title('Sums of Error Ratings across 207 Web-Scraped Training Examples')    \n",
    "\n",
    "for index, value in enumerate(rating_sums):\n",
    "    plt.text(index - 0.08, value -50, str(value), color='white') # add annotations\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "type_count = np.zeros((5, 10), dtype=int)\n",
    "\n",
    "for filename in video_data:\n",
    "    type_count[0][int(filename[4])] += 1\n",
    "    type_count[1][int(filename[7])] += 1\n",
    "    type_count[2][int(filename[10])] += 1\n",
    "    type_count[3][int(filename[13])] += 1\n",
    "    type_count[4][int(filename[16])] += 1\n",
    "\n",
    "# print(type_count)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(type_count, cmap='plasma')\n",
    "\n",
    "ax.set_xticks(np.arange(len(ratings)))\n",
    "ax.set_yticks(np.arange(len(errors)))\n",
    "ax.set_xticklabels(ratings)\n",
    "ax.set_yticklabels(errors)\n",
    "\n",
    "for i in range(len(errors)):\n",
    "    for j in range(len(ratings)):\n",
    "        text = ax.text(j, i, type_count[i, j], ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use MoveNet to Estimate Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing keypoints\n",
    "def draw_keypoints(frame, keypoints, min_confidence):\n",
    "    y, x, c = frame.shape\n",
    "    print ('height: ' + str(y) + ' width: ' + str(x))\n",
    "    print('KEYPOINTS:')\n",
    "    print(keypoints)\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [1, 1, 1]))\n",
    "\n",
    "    print(shaped[0])\n",
    "\n",
    "    for kp in shaped:\n",
    "        ky, kx, conf = kp\n",
    "        print('KY, KX, CONF: ' + str(int(ky * y)) + ' ' +  str(int(kx * x)) + ' ' + str(conf))\n",
    "        if conf > min_confidence:\n",
    "            cv2.circle(frame, (int(kx * x), int(ky * y)), 5, (255, 0, 0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose estimation with MoveNet\n",
    "# MEDIAPIPE (BELOW) GIVES BETTER RESULTS, SO MOVENET IS NOT USED\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_thunder_tflite_float16_4.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "filename = '..\\\\data_scraping\\\\Web_Scrape_Labeled_2\\\\ -- .mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    red = frame[:,:,2].copy()\n",
    "    blue = frame[:,:,0].copy()\n",
    "    frame[:,:,2] = blue\n",
    "    frame[:,:,0] = red\n",
    "\n",
    "    if ret:\n",
    "    \n",
    "        # reshape image\n",
    "        img = frame.copy()\n",
    "        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 256,256)\n",
    "        input_image = tf.cast(img, dtype=tf.uint8) # change base on lightning/thunder float32/uint8\n",
    "        \n",
    "        # set up input / output\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        # make prediction\n",
    "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "        interpreter.invoke()\n",
    "        points_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        print(points_with_scores)\n",
    "\n",
    "        # display prediction\n",
    "        square_img = np.array(tf.cast(np.squeeze(img), dtype=tf.uint8))\n",
    "\n",
    "        square_frame = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 1280,1280)\n",
    "        square_frame = np.array(tf.cast(np.squeeze(square_frame), dtype=tf.uint8))\n",
    "\n",
    "\n",
    "        draw_keypoints(square_frame, points_with_scores, 0.1)  \n",
    "        \n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.imshow(square_frame) # display only\n",
    "\n",
    "        cap.release()     \n",
    "    \n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use MediaPipe to Estimate Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data formatting\n",
    "\n",
    "def mediapipe_to_series(series, landmarks, height, width):\n",
    "\n",
    "    landmark_index = 0\n",
    "\n",
    "    for key in series:\n",
    "        if (key[-1] == 'x'): \n",
    "            series[key].append(landmarks[landmark_index].x * width)\n",
    "        elif (key[-1] == 'y'):\n",
    "            series[key].append(landmarks[landmark_index].y * height)\n",
    "        elif (key[-1] == 'z'):\n",
    "            series[key].append(landmarks[landmark_index].z)\n",
    "        else:\n",
    "            series[key].append(landmarks[landmark_index].visibility)\n",
    "            landmark_index += 1\n",
    "\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEDIAPIPE PERFORMS BETTER THAN MOVENET\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "ws_filepath = '..\\\\data_scraping\\\\Web_Scrape_Labeled_2\\\\'\n",
    "sr_filepath = '..\\\\data_supplement\\\\Self_Record_Labeled\\\\'\n",
    "\n",
    "web_scrape_videos = os.listdir(ws_filepath)\n",
    "self_record_videos = os.listdir(sr_filepath)\n",
    "\n",
    "\n",
    "for ws_video in web_scrape_videos:\n",
    "\n",
    "    filepath = ws_filepath + ws_video\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "    print (ws_video)\n",
    "    size = (int(w), int(h))\n",
    "\n",
    "    series = {\n",
    "            'left_shoulder_x' : [], 'left_shoulder_y' : [], 'left_shoulder_z' : [], 'left_shoulder_v' : [],\n",
    "            'right_shoulder_x' : [], 'right_shoulder_y' : [], 'right_shoulder_z' : [], 'right_shoulder_v' : [],\n",
    "            'left_elbow_x' : [], 'left_elbow_y' : [], 'left_elbow_z' : [], 'left_elbow_v' : [],\n",
    "            'right_elbow_x' : [], 'right_elbow_y' : [], 'right_elbow_z' : [], 'right_elbow_v' : [],\n",
    "            'left_wrist_x' : [], 'left_wrist_y' : [], 'left_wrist_z' : [], 'left_wrist_v' : [],\n",
    "            'right_wrist_x' : [], 'right_wrist_y' : [], 'right_wrist_z' : [], 'right_wrist_v' : [],\n",
    "            'left_hip_x' : [], 'left_hip_y' : [], 'left_hip_z' : [], 'left_hip_v' : [],\n",
    "            'right_hip_x' : [], 'right_hip_y' : [], 'right_hip_z' : [], 'right_hip_v' : [],\n",
    "            'left_knee_x' : [], 'left_knee_y' : [], 'left_knee_z' : [], 'left_knee_v' : [],\n",
    "            'right_knee_x' : [], 'right_knee_y' : [], 'right_knee_z' : [], 'right_knee_v' : [],\n",
    "            'left_ankle_x' : [], 'left_ankle_y' : [], 'left_ankle_z' : [], 'left_ankle_v' : [],\n",
    "            'right_ankle_x' : [], 'right_ankle_y' : [], 'right_ankle_z' : [], 'right_ankle_v' : [],\n",
    "            'left_heel_x' : [], 'left_heel_y' : [], 'left_heel_z' : [], 'left_heel_v' : [],\n",
    "            'right_heel_x' : [], 'right_heel_y' : [], 'right_heel_z' : [], 'right_heel_v' : [],\n",
    "            'left_foot_index_x' : [], 'left_foot_index_y' : [], 'left_foot_index_z' : [], 'left_foot_index_v' : [],\n",
    "            'right_foot_index_x' : [], 'right_foot_index_y' : [], 'right_foot_index_z' : [], 'right_foot_index_v' : []\n",
    "        }\n",
    "    \n",
    "    with open('..\\\\time_series\\\\web_scrape\\\\' + os.path.splitext(ws_video)[0] + '.csv', 'w') as csv_out:\n",
    "\n",
    "        pose_img_video = cv2.VideoWriter('..\\\\time_series\\\\pose_videos\\\\' + str(ws_video), cv2.VideoWriter_fourcc(*'h264'), frame_rate, size)\n",
    "        writer = csv.writer(csv_out)\n",
    "\n",
    "        with mp_pose.Pose(min_detection_confidence=0.1, min_tracking_confidence=0.1) as pose:\n",
    "            while(cap.isOpened()):\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "\n",
    "                    # convert color space for model\n",
    "                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "                    image.flags.writeable = False\n",
    "\n",
    "                    # invoke MediaPipe pose estimation\n",
    "                    results = pose.process(image)\n",
    "                    # print(results)\n",
    "\n",
    "                    # extract landmark coordinates\n",
    "                    try:\n",
    "                        landmarks = results.pose_landmarks.landmark\n",
    "                        #print(landmarks)\n",
    "                        series = mediapipe_to_series(series, landmarks, h, w)\n",
    "                    except:\n",
    "                        print('\\n\\nFAILED ON ' + str(ws_video) + '\\n\\n')\n",
    "                        pass\n",
    "                    \n",
    "                    # print(series)\n",
    "\n",
    "                    # visualize pose estimate\n",
    "                    image.flags.writeable = True\n",
    "                    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  \n",
    "                    pose_img_video.write(image)\n",
    "                    #plt.figure(figsize=(20,20))\n",
    "                    #plt.imshow(image)\n",
    "\n",
    "                else:\n",
    "                    # save video and csv\n",
    "                    pose_img_video.release()\n",
    "                    cap.release()\n",
    "\n",
    "                    writer.writerow(series.keys())\n",
    "                    writer.writerows(zip(*series.values()))\n",
    "\n",
    "                    # np.save('..\\\\time_series\\\\supplement\\\\' + os.path.splitext(sr_video)[0], series)\n",
    "                    break\n",
    "\n",
    "    print ('completed ' + ws_video)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59acfcc823b36a638b37cb1c7843aa46684cb4b3e7f7aef341e5384d13f48e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
