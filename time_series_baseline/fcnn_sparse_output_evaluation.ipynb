{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def series_append(series, list, keys):\n",
    "    for i in range(64):\n",
    "        series[keys[i]].append(float(list[i]))\n",
    "    return series\n",
    "\n",
    "def load_series(filename):\n",
    "    with open(filename, 'r') as csv_in:\n",
    "        csv_file = list(csv.reader(csv_in))\n",
    "        series = {}\n",
    "        keys = csv_file[0]\n",
    "        for key in keys: series[key] = []\n",
    "        for i in range(2, len(csv_file), 2):\n",
    "            series = series_append(series, csv_file[i], keys)\n",
    "        return [series, int((len(csv_file) - 2) / 2)]\n",
    "\n",
    "def unroll(series):\n",
    "    l = []\n",
    "    for key in series:\n",
    "        if (key[-1] == 'v' or key[-1] == 'z'): continue\n",
    "        l += (series[key])\n",
    "    return l\n",
    "\n",
    "def sparse_output(filename):\n",
    "    o_index = int(filename[4])\n",
    "    f_index = int(filename[7])\n",
    "    b_index = int(filename[10])\n",
    "    s_index = int(filename[13])\n",
    "    l_index = int(filename[16])\n",
    "    output = [0.0] * 50\n",
    "    output[o_index] = 1\n",
    "    output[f_index + 10] = 1.0\n",
    "    output[b_index + 20] = 1.0\n",
    "    output[s_index + 30] = 1.0\n",
    "    output[l_index + 40] = 1.0\n",
    "    return output\n",
    "\n",
    "# TODO define sparse error functions\n",
    "\n",
    "def sparseE0(predictions, truths):\n",
    "    # returns vector of length 5 with E0 for one type of data processing\n",
    "    o_num_correct = 0\n",
    "    f_num_correct = 0\n",
    "    b_num_correct = 0\n",
    "    s_num_correct = 0\n",
    "    l_num_correct = 0\n",
    "    n = len(predictions)\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "\n",
    "        truth = [int(j) for j in truths[i]]\n",
    "\n",
    "        if (pred[truth[0]] > 0.5): # if overstride correct\n",
    "            o_num_correct+=1\n",
    "        if (pred[truth[1] + 10] > 0.5): # if forward lean correct\n",
    "            f_num_correct+=1\n",
    "        if (pred[truth[2] + 20] > 0.5): # if backward lean correct\n",
    "            b_num_correct+=1 \n",
    "        if (pred[truth[3] + 30] > 0.5): # if sweeping correct\n",
    "            s_num_correct+=1\n",
    "        if (pred[truth[4] + 40] > 0.5): # if low arms correct\n",
    "            l_num_correct+=1\n",
    "    return [float(o_num_correct) / n, float(f_num_correct) / n, float(b_num_correct) / n, float(s_num_correct) / n, float(l_num_correct) / n]\n",
    "\n",
    "\n",
    "\n",
    "def sparseE1(predictions, truths):\n",
    "    # returns vector of length 5 with E0 for one type of data processing\n",
    "    o_num_correct = 0\n",
    "    f_num_correct = 0\n",
    "    b_num_correct = 0\n",
    "    s_num_correct = 0\n",
    "    l_num_correct = 0\n",
    "    n = len(predictions)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "\n",
    "        truth = [int(j) for j in truths[i]]\n",
    "\n",
    "        o_space = np.arange(max(truth[0] - 1, 0), min(truth[0] + 2, 9))\n",
    "        f_space = np.arange(max(truth[1] + 10 - 1, 10), min(truth[1] + 10 + 2, 19))\n",
    "        b_space = np.arange(max(truth[2] + 20 - 1, 20), min(truth[2] + 20 + 2, 29))\n",
    "        s_space = np.arange(max(truth[3] + 30 - 1, 30), min(truth[3] + 30 + 2, 39))\n",
    "        l_space = np.arange(max(truth[4] + 40 - 1, 40), min(truth[4] + 40 + 2, 49))\n",
    "\n",
    "        for truth_index in o_space:\n",
    "            if (pred[truth_index] > 0.5): # if overstride correct\n",
    "                o_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in f_space:\n",
    "            if (pred[truth_index] > 0.5): # if forward lean correct\n",
    "                f_num_correct+=1\n",
    "                break\n",
    "        \n",
    "        for truth_index in b_space:\n",
    "            if (pred[truth_index] > 0.5): # if backward lean correct\n",
    "                b_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in s_space:\n",
    "            if (pred[truth_index] > 0.5): # if sweeping correct\n",
    "                s_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in l_space:\n",
    "            if (pred[truth_index] > 0.5): # if low arms correct\n",
    "                l_num_correct+=1\n",
    "                break\n",
    "        \n",
    "    return [float(o_num_correct) / n, float(f_num_correct) / n, float(b_num_correct) / n, float(s_num_correct) / n, float(l_num_correct) / n]\n",
    "\n",
    "\n",
    "\n",
    "def sparseE2(predictions, truths):\n",
    "    # returns vector of length 5 with E0 for one type of data processing\n",
    "    o_num_correct = 0\n",
    "    f_num_correct = 0\n",
    "    b_num_correct = 0\n",
    "    s_num_correct = 0\n",
    "    l_num_correct = 0\n",
    "    n = len(predictions)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "\n",
    "        truth = [int(j) for j in truths[i]]\n",
    "\n",
    "        o_space = np.arange(max(truth[0] - 2, 0), min(truth[0] + 3, 9))\n",
    "        f_space = np.arange(max(truth[1] + 10 - 2, 10), min(truth[1] + 10 + 3, 19))\n",
    "        b_space = np.arange(max(truth[2] + 20 - 2, 20), min(truth[2] + 20 + 3, 29))\n",
    "        s_space = np.arange(max(truth[3] + 30 - 2, 30), min(truth[3] + 30 + 3, 39))\n",
    "        l_space = np.arange(max(truth[4] + 40 - 2, 40), min(truth[4] + 40 + 3, 49))\n",
    "\n",
    "        for truth_index in o_space:\n",
    "            if (pred[truth_index] > 0.5): # if overstride correct\n",
    "                o_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in f_space:\n",
    "            if (pred[truth_index] > 0.5): # if forward lean correct\n",
    "                f_num_correct+=1\n",
    "                break\n",
    "        \n",
    "        for truth_index in b_space:\n",
    "            if (pred[truth_index] > 0.5): # if backward lean correct\n",
    "                b_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in s_space:\n",
    "            if (pred[truth_index] > 0.5): # if sweeping correct\n",
    "                s_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in l_space:\n",
    "            if (pred[truth_index] > 0.5): # if low arms correct\n",
    "                l_num_correct+=1\n",
    "                break\n",
    "        \n",
    "    return [float(o_num_correct) / n, float(f_num_correct) / n, float(b_num_correct) / n, float(s_num_correct) / n, float(l_num_correct) / n]\n",
    "\n",
    "\n",
    "\n",
    "def sparseE3(predictions, truths):\n",
    "    # returns vector of length 5 with E0 for one type of data processing\n",
    "    o_num_correct = 0\n",
    "    f_num_correct = 0\n",
    "    b_num_correct = 0\n",
    "    s_num_correct = 0\n",
    "    l_num_correct = 0\n",
    "    n = len(predictions)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        \n",
    "        truth = [int(j) for j in truths[i]]\n",
    "\n",
    "        o_space = np.arange(max(truth[0] - 3, 0), min(truth[0] + 4, 9))\n",
    "        f_space = np.arange(max(truth[1] + 10 - 3, 10), min(truth[1] + 10 + 4, 19))\n",
    "        b_space = np.arange(max(truth[2] + 20 - 3, 20), min(truth[2] + 20 + 4, 29))\n",
    "        s_space = np.arange(max(truth[3] + 30 - 3, 30), min(truth[3] + 30 + 4, 39))\n",
    "        l_space = np.arange(max(truth[4] + 40 - 3, 40), min(truth[4] + 40 + 4, 49))\n",
    "\n",
    "        for truth_index in o_space:\n",
    "            if (pred[truth_index] > 0.5): # if overstride correct\n",
    "                o_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in f_space:\n",
    "            if (pred[truth_index] > 0.5): # if forward lean correct\n",
    "                f_num_correct+=1\n",
    "                break\n",
    "        \n",
    "        for truth_index in b_space:\n",
    "            if (pred[truth_index] > 0.5): # if backward lean correct\n",
    "                b_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in s_space:\n",
    "            if (pred[truth_index] > 0.5): # if sweeping correct\n",
    "                s_num_correct+=1\n",
    "                break\n",
    "\n",
    "        for truth_index in l_space:\n",
    "            if (pred[truth_index] > 0.5): # if low arms correct\n",
    "                l_num_correct+=1\n",
    "                break\n",
    "        \n",
    "    return [float(o_num_correct) / n, float(f_num_correct) / n, float(b_num_correct) / n, float(s_num_correct) / n, float(l_num_correct) / n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([93, 224])\n",
      "torch.Size([93, 224])\n",
      "torch.Size([93, 224])\n",
      "torch.Size([93, 224])\n",
      "torch.Size([93, 50])\n",
      "torch.Size([93, 5])\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([371, 224])\n",
      "torch.Size([371, 224])\n",
      "torch.Size([371, 224])\n",
      "torch.Size([371, 224])\n",
      "torch.Size([371, 50])\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "\n",
    "raw_X_train_list = []\n",
    "smoothed_X_train_list = []\n",
    "trans_X_train_list = []\n",
    "final_X_train_list = []\n",
    "\n",
    "y_train_list = []\n",
    "\n",
    "raw_X_test_list = []\n",
    "smoothed_X_test_list = []\n",
    "trans_X_test_list = []\n",
    "final_X_test_list = []\n",
    "\n",
    "y_test_list = []\n",
    "y_sparse_test_list = []\n",
    "\n",
    "with open(\"..\\\\test_examples.txt\") as t:\n",
    "    test_examples = t.readlines()\n",
    "\n",
    "for example in test_examples:\n",
    "    raw_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\1_unprocessed\\\\\" + example[:-1])\n",
    "    smoothed_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\2_smoothed\\\\\" + example[:-1])\n",
    "    trans_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\3_translation\\\\\" + example[:-1])\n",
    "    final_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\4_final\\\\\" + example[:-1])\n",
    "\n",
    "    raw_X_test_list.append(unroll(raw_series))\n",
    "    smoothed_X_test_list.append(unroll(smoothed_series))\n",
    "    trans_X_test_list.append(unroll(trans_series))\n",
    "    final_X_test_list.append(unroll(final_series))\n",
    "    \n",
    "    y_test_list.append([int(example[4]), int(example[7]), int(example[10]), int(example[13]), int(example[16])])\n",
    "    y_sparse_test_list.append(sparse_output(example))\n",
    "\n",
    "raw_X_test = torch.tensor(raw_X_test_list)\n",
    "smoothed_X_test = torch.tensor(smoothed_X_test_list)\n",
    "trans_X_test = torch.tensor(trans_X_test_list)\n",
    "final_X_test = torch.tensor(final_X_test_list)\n",
    "\n",
    "y_sparse_test = torch.tensor(y_sparse_test_list)\n",
    "y_test = torch.tensor(y_test_list)\n",
    "\n",
    "print(raw_X_test.size())\n",
    "print(smoothed_X_test.size())\n",
    "print(trans_X_test.size())\n",
    "print(final_X_test.size())\n",
    "print(y_sparse_test.size())\n",
    "print(y_test.size())\n",
    "print('\\n\\n')\n",
    "\n",
    "with open(\"..\\\\training_examples.txt\") as t:\n",
    "    training_examples = t.readlines()\n",
    "\n",
    "for example in training_examples:\n",
    "    raw_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\1_unprocessed\\\\\" + example[:-1])\n",
    "    smoothed_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\2_smoothed\\\\\" + example[:-1])\n",
    "    trans_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\3_translation\\\\\" + example[:-1])\n",
    "    final_series, num_frames = load_series(\"..\\\\time_series\\\\Time_normalized_stages\\\\4_final\\\\\" + example[:-1])\n",
    "    y_example = sparse_output(example)\n",
    "\n",
    "    raw_X_train_list.append(unroll(raw_series))\n",
    "    smoothed_X_train_list.append(unroll(smoothed_series))\n",
    "    trans_X_train_list.append(unroll(trans_series))\n",
    "    final_X_train_list.append(unroll(final_series))\n",
    "    \n",
    "    y_train_list.append(y_example)\n",
    "\n",
    "raw_X_train = torch.tensor(raw_X_train_list)\n",
    "smoothed_X_train = torch.tensor(smoothed_X_train_list)\n",
    "trans_X_train = torch.tensor(trans_X_train_list)\n",
    "final_X_train = torch.tensor(final_X_train_list)\n",
    "\n",
    "y_train = torch.tensor(y_train_list)\n",
    "\n",
    "print(raw_X_train.size())\n",
    "print(smoothed_X_train.size())\n",
    "print(trans_X_train.size())\n",
    "print(final_X_train.size())\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model, parameters, and weight initialization\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "input_size = 224\n",
    "hidden_layer_size_1 = 450\n",
    "hidden_layer_size_2 = 420\n",
    "hidden_layer_size_3 = 350\n",
    "hidden_layer_size_4 = 270\n",
    "hidden_layer_size_5 = 150\n",
    "hidden_layer_size_6 = 100\n",
    "output_size = 50\n",
    "batch_size = 371\n",
    "learning_rate = 0.00005\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_layer_size_1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_size_1, hidden_layer_size_2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_size_2, hidden_layer_size_3),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_size_3, hidden_layer_size_4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_size_4, hidden_layer_size_5),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_size_5, hidden_layer_size_6),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_layer_size_6, output_size),\n",
    "                        nn.Sigmoid())\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1854863166809082\n",
      "RAW E0: [0.07526881720430108, 0.7419354838709677, 0.16129032258064516, 0.08602150537634409, 0.043010752688172046]\n",
      "RAW E1: [0.27956989247311825, 0.8817204301075269, 0.26881720430107525, 0.3225806451612903, 0.1827956989247312]\n",
      "RAW E2: [0.41935483870967744, 0.9139784946236559, 0.5376344086021505, 0.5483870967741935, 0.45161290322580644]\n",
      "RAW E3: [0.45161290322580644, 0.9354838709677419, 0.5376344086021505, 0.6774193548387096, 0.45161290322580644]\n"
     ]
    }
   ],
   "source": [
    "# training and evaluating raw series\n",
    "\n",
    "min_loss = 1.0\n",
    "\n",
    "training_cycles = 20\n",
    "\n",
    "raw_E0 = []\n",
    "raw_E1 = []\n",
    "raw_E2 = []\n",
    "raw_E3 = []\n",
    "\n",
    "for i in range(training_cycles):\n",
    "    model.apply(init_weights)\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pred_y = model(raw_X_train)\n",
    "        loss = loss_function(pred_y, y_train)\n",
    "\n",
    "        test_y = model(raw_X_test)\n",
    "        test_loss = loss_function(test_y, y_sparse_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if (min(test_losses) < min_loss):\n",
    "        min_loss = min(test_losses)\n",
    "        test_y = model(raw_X_test)\n",
    "\n",
    "        raw_E0.clear()\n",
    "        raw_E1.clear()\n",
    "        raw_E2.clear()\n",
    "        raw_E3.clear()\n",
    "\n",
    "        raw_E0 = sparseE0(test_y, y_test.tolist())\n",
    "        raw_E1 = sparseE1(test_y, y_test.tolist())\n",
    "        raw_E2 = sparseE2(test_y, y_test.tolist())\n",
    "        raw_E3 = sparseE3(test_y, y_test.tolist())\n",
    "\n",
    "print('MSE: ' + str(min_loss))\n",
    "print('RAW E0: ' + str(raw_E0))\n",
    "print('RAW E1: ' + str(raw_E1))\n",
    "print('RAW E2: ' + str(raw_E2))\n",
    "print('RAW E3: ' + str(raw_E3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.19392509758472443\n",
      "SMOOTHED E0: [0.17204301075268819, 0.6989247311827957, 0.24731182795698925, 0.043010752688172046, 0.08602150537634409]\n",
      "SMOOTHED E1: [0.3978494623655914, 0.8172043010752689, 0.3225806451612903, 0.12903225806451613, 0.24731182795698925]\n",
      "SMOOTHED E2: [0.5053763440860215, 0.8387096774193549, 0.4838709677419355, 0.1827956989247312, 0.5268817204301075]\n",
      "SMOOTHED E3: [0.5483870967741935, 0.967741935483871, 0.6129032258064516, 0.23655913978494625, 0.5483870967741935]\n"
     ]
    }
   ],
   "source": [
    "# training and evaluating smoothed series\n",
    "\n",
    "min_loss = 1.0\n",
    "\n",
    "training_cycles = 20\n",
    "\n",
    "smooth_E0 = []\n",
    "smooth_E1 = []\n",
    "smooth_E2 = []\n",
    "smooth_E3 = []\n",
    "\n",
    "for i in range(training_cycles):\n",
    "    model.apply(init_weights)\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pred_y = model(smoothed_X_train)\n",
    "        loss = loss_function(pred_y, y_train)\n",
    "\n",
    "        test_y = model(smoothed_X_test)\n",
    "        test_loss = loss_function(test_y, y_sparse_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if (min(test_losses) < min_loss):\n",
    "        min_loss = min(test_losses)\n",
    "        test_y = model(smoothed_X_test)\n",
    "\n",
    "        smooth_E0.clear()\n",
    "        smooth_E1.clear()\n",
    "        smooth_E2.clear()\n",
    "        smooth_E3.clear()\n",
    "\n",
    "        smooth_E0 = sparseE0(test_y, y_test.tolist())\n",
    "        smooth_E1 = sparseE1(test_y, y_test.tolist())\n",
    "        smooth_E2 = sparseE2(test_y, y_test.tolist())\n",
    "        smooth_E3 = sparseE3(test_y, y_test.tolist())\n",
    "\n",
    "print('MSE: ' + str(min_loss))\n",
    "print('SMOOTHED E0: ' + str(smooth_E0))\n",
    "print('SMOOTHED E1: ' + str(smooth_E1))\n",
    "print('SMOOTHED E2: ' + str(smooth_E2))\n",
    "print('SMOOTHED E3: ' + str(smooth_E3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1247793659567833\n",
      "TRANS E0: [0.03225806451612903, 0.6021505376344086, 0.44086021505376344, 0.053763440860215055, 0.27956989247311825]\n",
      "TRANS E1: [0.08602150537634409, 0.7311827956989247, 0.5268817204301075, 0.08602150537634409, 0.41935483870967744]\n",
      "TRANS E2: [0.11827956989247312, 0.8709677419354839, 0.6236559139784946, 0.08602150537634409, 0.5268817204301075]\n",
      "TRANS E3: [0.11827956989247312, 0.9354838709677419, 0.6881720430107527, 0.0967741935483871, 0.6451612903225806]\n"
     ]
    }
   ],
   "source": [
    "# training and evaluating translationally normalized series\n",
    "\n",
    "min_loss = 1.0\n",
    "\n",
    "training_cycles = 20\n",
    "\n",
    "trans_E0 = []\n",
    "trans_E1 = []\n",
    "trans_E2 = []\n",
    "trans_E3 = []\n",
    "\n",
    "for i in range(training_cycles):\n",
    "    model.apply(init_weights)\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pred_y = model(trans_X_train)\n",
    "        loss = loss_function(pred_y, y_train)\n",
    "\n",
    "        test_y = model(trans_X_test)\n",
    "        test_loss = loss_function(test_y, y_sparse_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if (min(test_losses) < min_loss):\n",
    "        min_loss = min(test_losses)\n",
    "        test_y = model(trans_X_test)\n",
    "\n",
    "        trans_E0.clear()\n",
    "        trans_E1.clear()\n",
    "        trans_E2.clear()\n",
    "        trans_E3.clear()\n",
    "\n",
    "        trans_E0 = sparseE0(test_y, y_test.tolist())\n",
    "        trans_E1 = sparseE1(test_y, y_test.tolist())\n",
    "        trans_E2 = sparseE2(test_y, y_test.tolist())\n",
    "        trans_E3 = sparseE3(test_y, y_test.tolist())\n",
    "\n",
    "print('MSE: ' + str(min_loss))\n",
    "print('TRANS E0: ' + str(trans_E0))\n",
    "print('TRANS E1: ' + str(trans_E1))\n",
    "print('TRANS E2: ' + str(trans_E2))\n",
    "print('TRANS E3: ' + str(trans_E3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.17096808552742004\n",
      "FINAL E0: [0.0, 0.7634408602150538, 0.44086021505376344, 0.021505376344086023, 0.0]\n",
      "FINAL E1: [0.0, 0.9247311827956989, 0.5268817204301075, 0.043010752688172046, 0.0]\n",
      "FINAL E2: [0.0, 1.0, 0.6236559139784946, 0.17204301075268819, 0.0]\n",
      "FINAL E3: [0.0, 1.0, 0.6881720430107527, 0.3655913978494624, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# training and evaluating final series\n",
    "\n",
    "min_loss = 1.0\n",
    "\n",
    "training_cycles = 20\n",
    "\n",
    "final_E0 = []\n",
    "final_E1 = []\n",
    "final_E2 = []\n",
    "final_E3 = []\n",
    "\n",
    "for i in range(training_cycles):\n",
    "    model.apply(init_weights)\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pred_y = model(final_X_train)\n",
    "        loss = loss_function(pred_y, y_train)\n",
    "\n",
    "        test_y = model(final_X_test)\n",
    "        test_loss = loss_function(test_y, y_sparse_test)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if (min(test_losses) < min_loss):\n",
    "        min_loss = min(test_losses)\n",
    "        test_y = model(final_X_test)\n",
    "\n",
    "        final_E0.clear()\n",
    "        final_E1.clear()\n",
    "        final_E2.clear()\n",
    "        final_E3.clear()\n",
    "\n",
    "        final_E0 = sparseE0(test_y, y_test.tolist())\n",
    "        final_E1 = sparseE1(test_y, y_test.tolist())\n",
    "        final_E2 = sparseE2(test_y, y_test.tolist())\n",
    "        final_E3 = sparseE3(test_y, y_test.tolist())\n",
    "\n",
    "print('MSE: ' + str(min_loss))\n",
    "print('FINAL E0: ' + str(final_E0))\n",
    "print('FINAL E1: ' + str(final_E1))\n",
    "print('FINAL E2: ' + str(final_E2))\n",
    "print('FINAL E3: ' + str(final_E3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left_shoulder_x': [-45.109706257588044, 155.48292136690202, 188.64269610476435, 30.13477080729921, 26.15070618718543, -68.70381184544468, -16.733536172175175], 'left_shoulder_y': [1433.57148915248, 1387.5110596464683, 1382.0930323177479, 1332.333879700497, 1380.012570094935, 1447.2141253144591, 1397.5924169719626], 'left_shoulder_z': [-0.09956710917274936, -0.08924960140043098, -0.03575645609883142, -0.07751576827500217, -0.07193020773619137, -0.05725977160701786, -0.10525742901244198], 'left_shoulder_v': [0.9999894499778748, 0.9999874830245972, 0.9999619722366333, 0.9999496936798096, 0.9999605417251587, 0.9999725818634033, 0.9999803304672241], 'right_shoulder_x': [160.2851192557694, 105.39241581115354, 30.51511345491679, 20.483510064475144, 87.04708302286144, -41.78909919001301, -136.72306340039285], 'right_shoulder_y': [1410.143913334849, 1341.3245131443864, 1269.566923118981, 1255.7151894274402, 1302.9415789401942, 1359.8634687573258, 1352.6648491919825], 'right_shoulder_z': [0.08046816918962157, 0.09147934825839946, 0.10523933577251093, 0.10962207937435844, 0.12336131641202795, 0.14542285199236765, 0.09397984456217498], 'right_shoulder_v': [0.999885618686676, 0.9998050928115845, 0.9997597932815552, 0.9998272657394409, 0.9998319149017334, 0.999771237373352, 0.9995213747024536], 'left_elbow_x': [-170.57047096524946, 174.2577696131439, 260.0980846614596, -44.5378944616656, -180.8554290899783, -258.0855872285965, -0.3875307702551616], 'left_elbow_y': [914.7009694039112, 880.2537460011054, 867.5173571074436, 888.2761794014501, 1022.7600882867685, 1085.7672011849847, 889.5597083121321], 'left_elbow_z': [-0.12344981716692255, -0.06691715518215227, -0.014790720156754518, -0.13319196382980825, -0.14935248018241387, -0.1386199492156842, -0.10938580684465626], 'left_elbow_v': [0.9928616881370544, 0.990289568901062, 0.9804368615150452, 0.977761447429657, 0.983402669429779, 0.9881618618965149, 0.989177405834198], 'right_elbow_x': [106.02458447580287, -48.78488232004797, -244.2808387487793, -45.815384236251354, 128.8957755729306, -84.1553522305024, -280.2817644695681], 'right_elbow_y': [864.3497908997872, 822.0524124841081, 875.3026123330261, 806.7317110152804, 824.1006712189318, 871.0997214621889, 789.8529177211108], 'right_elbow_z': [0.10255125146225641, 0.0997708250878849, 0.09850331442874719, 0.1515540426057568, 0.17607782740328334, 0.19985249588060372, 0.13290316813592112], 'right_elbow_v': [0.3687764108181, 0.29564204812049866, 0.28321805596351624, 0.22821536660194397, 0.19278642535209656, 0.15923713147640228, 0.1267545372247696], 'left_wrist_x': [335.7606953400992, 493.10679775577717, 342.0093830641302, 306.25475077725196, 251.2233349259998, 172.73196576601015, 390.95060246528783], 'left_wrist_y': [879.7568558274961, 1016.2305566132829, 872.5317931042451, 630.5264958071211, 765.2695468353409, 908.0049626153789, 783.0368792719725], 'left_wrist_z': [-0.0979820442733956, 0.03009416205476908, 0.07520852193027322, -0.14780520769190783, -0.18176939911425724, -0.16703944761826112, -0.03978164214565255], 'left_wrist_v': [0.9679811596870422, 0.9590973258018494, 0.7929396033287048, 0.7996672987937927, 0.8527755737304688, 0.9009252190589905, 0.9143269062042236], 'right_wrist_x': [303.8004714530086, 9.108540184449506, -157.08222164587255, 87.15082386555221, 410.93195472473934, 166.24481154954327, -133.56302489424067], 'right_wrist_y': [545.2425882822652, 592.0095903459485, 827.7134825730295, 540.3932396846443, 771.571468388848, 820.1268398776214, 544.9114024567474], 'right_wrist_z': [0.04802140463493834, 0.059881839890877565, 0.05807860816760989, 0.14283367342674538, 0.17784574245192383, 0.21240706710664936, 0.13273135823116003], 'right_wrist_v': [0.5414425730705261, 0.42706823348999023, 0.3274102509021759, 0.2566834092140198, 0.2784332036972046, 0.3138061761856079, 0.22334519028663635], 'left_hip_x': [-17.57246283134779, -30.957492893996214, 61.922196528242594, 27.31887342212923, 13.182750044516022, 52.57870428361749, 45.03044175726234], 'left_hip_y': [418.79600844126713, 350.4489318117212, 295.0442294784747, 292.11124816394215, 352.44833722908726, 374.5813943171743, 343.41297398253465], 'left_hip_z': [-0.0701480265405659, -0.07761119295831848, -0.0767818014254302, -0.0683991924717654, -0.06885761619738023, -0.06276645626392996, -0.09620966755374807], 'left_hip_v': [0.9998188614845276, 0.9998046159744263, 0.9998314380645752, 0.9998339414596558, 0.9998428821563721, 0.9997612833976746, 0.9997692108154297], 'right_hip_x': [125.14235400063278, 129.28678609011038, 171.22671460933955, 17.64669632192661, -27.24976188617594, -19.022706177128157, -35.88817313818835], 'right_hip_y': [410.76161108499326, 341.61002436822963, 279.51629547382925, 255.71921319164042, 309.4949365663627, 360.12265667192196, 357.76167555858365], 'right_hip_z': [0.048378958675110224, 0.041524405754221644, 0.027680453463782284, 0.04166041662465111, 0.04935331551838876, 0.06256430979427022, 0.03583615894597703], 'right_hip_v': [0.9994919896125793, 0.9992856979370117, 0.9995027780532837, 0.9996671080589294, 0.9997362494468689, 0.9994826912879944, 0.9980573654174805], 'left_knee_x': [188.72372675251876, -257.6115775327005, -291.2235824317632, 361.5851442390433, 763.947427868531, 640.1183541117972, 353.5585761007413], 'left_knee_y': [-458.0833919569563, -467.7817028891174, -518.7129889341217, -423.004595577022, -60.6431874643213, -272.51753759727694, -521.0573508410877], 'left_knee_z': [-0.08547541898490296, -0.10216983413606973, -0.09915876852759037, -0.09561820281943512, -0.07761752472064563, -0.06680735980071828, -0.09912451390036894], 'left_knee_v': [0.9923062324523926, 0.9938105344772339, 0.9952108860015869, 0.9946712255477905, 0.9937328100204468, 0.9921883940696716, 0.991607666015625], 'right_knee_x': [369.8087611145508, 727.434217730377, 578.4036462489753, 230.85503413301737, -126.98105059793235, -175.7672337555897, 388.45464382805335], 'right_knee_y': [-340.26020747301123, -135.64517483018818, -335.96782305697263, -557.2259006528253, -501.47269331930124, -469.0578232603247, -335.15637096360024], 'right_knee_z': [0.05847452152548128, 0.037543614356409606, 0.022584071881204523, 0.026012153682497742, 0.030661769248837414, 0.040401855260899705, 0.04099893354325956], 'right_knee_v': [0.8568207025527954, 0.8847813010215759, 0.9117306470870972, 0.9165521860122681, 0.8983736038208008, 0.8969148397445679, 0.8253629803657532], 'left_ankle_x': [-226.65524788432927, -763.9435016700229, -1030.494453639787, -327.5875264422295, 208.93052343734007, 846.1415385635629, 136.22658596319613], 'left_ankle_y': [-1121.3507433612529, -1160.9547725151524, -505.2433071331883, -135.61737480488802, -523.905466924681, -1269.3161388652172, -1435.377486884929], 'left_ankle_z': [-0.0573314653763202, -0.09508262215271876, -0.06883672625591815, -0.005995602450489505, -0.016558874256100517, -0.06681909279905862, -0.0939622506677188], 'left_ankle_v': [0.9780855178833008, 0.9837561845779419, 0.9882400631904602, 0.9775890707969666, 0.9694903492927551, 0.978794276714325, 0.9839589595794678], 'right_ankle_x': [-215.22160977262462, 256.32684852151243, 721.1198358174395, 25.549361659806298, -610.2793456392088, -978.9556555940301, -320.6593019032263], 'right_ankle_y': [-766.3547333893678, -660.6797510361024, -1251.0832331732854, -1439.533932343459, -1302.8488250960877, -704.4143449662432, -289.85633101996683], 'right_ankle_z': [0.11860109456636911, 0.10395441496784044, 0.02306128125396495, 0.019092543098302703, 0.01616417682246863, 0.028909658326755246, 0.10580767662585919], 'right_ankle_v': [0.9521604180335999, 0.9400317072868347, 0.958321213722229, 0.9673076272010803, 0.9567190408706665, 0.9548859000205994, 0.8539612889289856], 'left_heel_x': [-350.7222015124414, -926.5790819712838, -1113.575166998853, -380.68026577199436, -53.69767629387941, 760.7371568168641, 10.213640812413832], 'left_heel_y': [-1185.441946145932, -1253.1865442878675, -420.06387466163983, -25.381693888443127, -626.0880312056339, -1452.538502213358, -1577.1185285490558], 'left_heel_z': [-0.05506646213662456, -0.09449056877267786, -0.06310243270968081, 0.003600229234294809, -0.009495531872783685, -0.06745153768048551, -0.09420316636857656], 'left_heel_v': [0.9558776617050171, 0.9631644487380981, 0.970233678817749, 0.9530737400054932, 0.9496704339981079, 0.9627555012702942, 0.9685437679290771], 'right_heel_x': [-334.60436588726753, 132.60970607311134, 642.6896095193791, -71.5880505043083, -759.6771358313913, -1137.3082251620754, -435.26099720981125], 'right_heel_y': [-731.3335359772116, -694.4913257855985, -1433.9298148592093, -1565.4768776440806, -1403.5455992707432, -656.2445380698376, -213.22765685677678], 'right_heel_z': [0.12224513018749197, 0.10990167416211855, 0.020717871681192622, 0.01550409150261605, 0.012282153633090516, 0.027402042198595572, 0.11281185721836412], 'right_heel_v': [0.9607150554656982, 0.9450930953025818, 0.9564938545227051, 0.9622067809104919, 0.9445134997367859, 0.948471188545227, 0.9083037972450256], 'left_foot_index_x': [-61.24733181653545, -557.8732671024035, -1232.2048916568328, -588.3337064642551, 262.60068996255643, 1224.0133695615705, 472.1086130072658], 'left_foot_index_y': [-1273.3020197454912, -1436.7793978780253, -822.8932961928376, -246.21527755541445, -766.5551431268793, -1327.71914838756, -1571.1058128210104], 'left_foot_index_z': [-0.09379696303958895, -0.14768411597520198, -0.07005318499469965, 0.020452937336813165, -0.006259395803165623, -0.10294470022577631, -0.13237710720763696], 'left_foot_index_v': [0.9802228212356567, 0.9850429892539978, 0.9882431030273438, 0.9785826802253723, 0.970502495765686, 0.9781884551048279, 0.9817408323287964], 'right_foot_index_x': [-167.84231546501655, 402.74380034391766, 1072.2338751132836, 351.56386259020115, -394.16984640808215, -1098.778229469578, -437.0457119763644], 'right_foot_index_y': [-1001.196648377839, -921.922165193197, -1381.3913874955292, -1609.3515039258782, -1543.5402511528291, -1074.9723368412517, -515.8932855306039], 'right_foot_index_z': [0.10407677644980126, 0.09905480604504877, -0.00259336841038052, -0.0018062293476279398, -0.003905271607082237, 0.01274803465079047, 0.1152325864380841], 'right_foot_index_v': [0.9547871351242065, 0.9488446712493896, 0.9609701037406921, 0.9682068228721619, 0.9499452114105225, 0.94422847032547, 0.8801748752593994]}\n"
     ]
    }
   ],
   "source": [
    "final_files = os.listdir('..\\\\time_series\\\\Time_normalized_stages\\\\4_final\\\\')\n",
    "\n",
    "means = []\n",
    "std_devs = []\n",
    "mins = []\n",
    "maxes = []\n",
    "\n",
    "for file in final_files:\n",
    "    raw_series, num_frames = load_series('..\\\\time_series\\\\Time_normalized_stages\\\\1_unprocessed\\\\' + file)\n",
    "    smoothed_series, num_frames = load_series('..\\\\time_series\\\\Time_normalized_stages\\\\4_final\\\\' + file)\n",
    "    final_series, num_frames = load_series('..\\\\time_series\\\\Time_normalized_stages\\\\4_final\\\\' + file)\n",
    "    print(series)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59acfcc823b36a638b37cb1c7843aa46684cb4b3e7f7aef341e5384d13f48e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
