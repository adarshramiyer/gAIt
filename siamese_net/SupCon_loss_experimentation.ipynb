{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' https://github.com/HobbitLong/SupContrast/blob/master/losses.py '''\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up CNN\n",
    "\n",
    "conv_1 = 32\n",
    "conv_2 = 64\n",
    "conv_3 = 128\n",
    "conv_4 = 256\n",
    "\n",
    "fc_size1 = 256\n",
    "fc_size2 = 512\n",
    "fc_size3 = 256\n",
    "fc_size4 = 128\n",
    "fc_size5 = 64\n",
    "output_size = 32\n",
    "batch_size = 371\n",
    "learning_rate = 0.01\n",
    "tmp = 0.07\n",
    "    \n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(1.0)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.apply(self._init_weights)\n",
    "        # conv layers\n",
    "        self.conv1 = nn.Conv2d(conv_1, conv_2, (1,3), stride=1)\n",
    "        self.conv2 = nn.Conv2d(conv_2, conv_3, (1,3), stride=1)\n",
    "        self.conv3 = nn.Conv2d(conv_3, conv_4, (1,3), stride=1)\n",
    "        # fc layers\n",
    "        self.fc1 = nn.Linear(fc_size1, fc_size2)\n",
    "        self.fc2 = nn.Linear(fc_size2, fc_size3)\n",
    "        self.fc3 = nn.Linear(fc_size3, fc_size4)\n",
    "        self.fc4 = nn.Linear(fc_size4, fc_size5)\n",
    "        self.fc5 = nn.Linear(fc_size5, output_size)\n",
    "        # sigmoid\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convolutions\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.conv3(x), negative_slope=0.1)\n",
    "\n",
    "        #print('AFTER CONV: ' + str(x.size()))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.fc3(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.fc4(x), negative_slope=0.1)\n",
    "        x = F.leaky_relu(self.fc5(x), negative_slope=0.1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = SiameseNet()\n",
    "\n",
    "loss_function = SupConLoss(temperature=tmp)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59acfcc823b36a638b37cb1c7843aa46684cb4b3e7f7aef341e5384d13f48e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
